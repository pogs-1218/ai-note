housing price example
basic of machine learning

linesar regression

todo: 
download house price dataset

regression vs classfication

notation
x: input feature, 
y: output/target variable
m: nmber of training examples
this is supervised learning so that it has data and label
x is data, y is label
(x, y) is single training example which is data with a corresponding label.
x_i, y_i : ith training example.
x -> f -> y_hat
feature -> model -> prediction
How to represent f?
f_wb = WX+B

why linear function?

linear + regression!
with one variable(single feature)

f_wb = wx_b -> linear!
f_wb: taret(output) 

* the goal is to find w, b(parameters) that is close to y(target)
  (x, y) : train data is fixed!(supervised learning)

* cost function
- squared error cost function
if 0 <= i <= m
j_wb = np.sum((y_hat[i]-y[i])**2) / 2m


* why gradient descent?
start with some w, b
keep changing w, b to reduce J(w, b)
until minium
w = w - (alpha * dj_dw)

* about learning rate
if it is too large?
if it is too small?


linear regression with multiple features
- what's different with single feature linear regression
x_vec = [x1, x2, x3, ... xn]
w_vec = [w1, w2, w3, ... wn]
* b is scalar value.
f_wb = w1x1 + w2x2 + w3x3 + .. + b

-> so that how to compute those more efficiently?
f_wb = np.dot(x_vec, w_vec) + b
 * parelle! how? through numpy? how?

- why machine learning?

- linear regression(one variable)
  - ex. housing price prediction
  
  - Prediction
  - make a model

  - Evaluate
    - cost function

  - Train(Learning)
    - gradient descent
    - pyplot graph

- linear regression(multiple variable)
  - vectorization
  - polynomial regression
  - feature scaling
  - scikit-learn

- classfication with logisitic regression
  - logistic regression
  - decision boundary
  - cost function
  - gradient descent
  - overfitting
  - regularization
