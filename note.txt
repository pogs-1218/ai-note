# ai-note


Debugging a learning algorithm
---
Get more training examples -> high variance
try smaller set fo features -> high variance
try getting additional features -> high bias
adding polynomial features -> high bias
decreasing lambda(regularization term) -> high bias
increasing lambda(regularization term) -> high variance
What do you try next?


Evaluate a model
---
evaluate a performance

Split the data set
70% training set
30% test set

Training is done, then, compute the test error with test set.
In this case, no regularization.

Model selection
---
first-order polynomial
f = w1x + b
secod-order polynomial
f = w1x + w2x^2 + b
...

which one is the loweset cost value(J)

### Training/cross validation/test set
60% training set
20% cross validation
20% test set

How to use cross validation set?
why?

While model selection
Evaluate the parameter(w, b) in the validation set.


Bias and variance
---
High bias(underfit)
Jtrain is high and Jcv is high also.

High variance(overfit)
Jtrain is low but Jcv is high.

Regularization
large lambda -> underfit
small lambda -> overfit

Baseline level of performance
---
i.e) speech recognition example
human level performance: 10.6%
training error: 10.8%
cross validation error: 14.8%


baseline performance ~ training error
bias related.
training error ~ cross validation error
variance related.


The bias variance tradeoff
---
simple model <-----> complex model
(high bias)          (high variance)


Large neural networks are low bias machines.
repeat{
check bias in train set -> no -> bigger network
check variance in cv set -> no -> more data
}

Iterative loop of ML development
---
repeat{
choose architecture(model, data, etc)
-> train model
-> diagnostics(bias, variance, error analysis)
}

Error analysis
Manually examine the failed examples.
categorized them.


Add data

Data augmentation
: modifying an existing training examples to create a new training example.

Transfer learning
---
Use parameters pretrained in neural network with same type.(images, audio..)
Further train(fine tune) the network on your own data


Full cycle of a machine learning project
What steps?!
i.e) speech recognition 
Scope project -> 
repeat(
Collect data -> Train model ->
Deploy in production
)

Define project -> Define and collect data -> 
Training, error analysys & iter improve ->

Deployment
Inference server includes ML model
Mobile app(client) API call to Inference server
Inference server response to the client.


skewed dataset
---
precision and recall


Decision Tree Model
---
Create a hirarchy tree including each feature.
A lot of decision tree can be created.
how to choose it?
1. How to choose a root node?
2. What feature next??


Decision 1. How to choose what feature to split on at each node?
Maximize purity

Decision 2. When do you stop splitting?
When a node is 100% one class
When splitting a node will result int the tree exceeding a maximum depth
When improvements in pruty score are below a threshold

Measuring purity
---
Entropy as a measure of impurity
so, how to use this entropy to make a decision tree?

hmm, it is a binary search tree.
create a child node by using recursion.

Information Gain
---

One-hot encoding
---
Multiple, over two, possible values.

Continuous valued features
---

Regression with Decision Trees
---

Multiple decision trees
---
Trees are highly sensitive to small changes of the data

Tree ensemble
---
Sampling with replacement

Random forest algorithm
---
Tree ensemble algorithm
Generating a tree sample(sampling with replacement)
train a decision tree on new dataset

Will make different dicisiont trees

Randomizing the feature choice
pick a random subset of k<n features,
allow the algorithm to only choose from that subset of features.

XGBoost
---
vs random forest
: not a same probability(1/m)
: pick misclassified dataset from previously trained trees
: focus on smaller part

eXtreme Gradient Boosting

When to use decisinon tress?
vs neural networks

Decision tree and Tree ensembles
works well on tabular(structured) data
not recommended for unstructured data(images, audio, text)
fast
small decision tress may be human interpretable

Neural networks
works well on all types of data, including tabular and unstructured data
may be slower than a decision tree
works with transfer learning(can use pre-trained parameters)
multiple models working together
